{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1480377,"sourceType":"datasetVersion","datasetId":868818}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **<center> Constructing an LLM from Scratch<center>**\n#### The main motive of this notebook is to give a basic knowledge about the working of LLM. In this notebook I will be implementing a basic LLM from scratch and will be explaining it side by side .","metadata":{}},{"cell_type":"markdown","source":"## **Importing Modules**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(69)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-02-17T21:26:42.716676Z","iopub.execute_input":"2024-02-17T21:26:42.717441Z","iopub.status.idle":"2024-02-17T21:26:46.232350Z","shell.execute_reply.started":"2024-02-17T21:26:42.717412Z","shell.execute_reply":"2024-02-17T21:26:46.231204Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7d110fcef270>"},"metadata":{}}]},{"cell_type":"markdown","source":"## **Dataset**","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/input/shakespeare/shakespeare.txt', 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-02-17T21:26:46.234551Z","iopub.execute_input":"2024-02-17T21:26:46.235364Z","iopub.status.idle":"2024-02-17T21:26:46.251889Z","shell.execute_reply.started":"2024-02-17T21:26:46.235328Z","shell.execute_reply":"2024-02-17T21:26:46.251031Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## **Hyperparameter For The Model**","metadata":{}},{"cell_type":"code","source":"# Define hyperparameters for training a model\nbatch_size = 16  # how many independent sequences will we process in parallel?\nblock_size = 32  # what is the maximum context length for predictions?\nmax_iters = 5000  # maximum number of training iterations\neval_interval = 100  # evaluate the model every `eval_interval` iterations\nlearning_rate = 1e-3  # learning rate for the optimizer\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'  # use GPU if available, otherwise use CPU\neval_iters = 200  # number of iterations between each evaluation\nn_embd = 64  # dimensionality of the embedding layer\nn_head = 4  # number of attention heads\nn_layer = 4  # number of transformer layers\ndropout = 0.5  # dropout probability, set to 0.0 for no dropout\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:26:46.257360Z","iopub.execute_input":"2024-02-17T21:26:46.257781Z","iopub.status.idle":"2024-02-17T21:26:46.288495Z","shell.execute_reply.started":"2024-02-17T21:26:46.257749Z","shell.execute_reply":"2024-02-17T21:26:46.287445Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Brief explanation of each hyperparameter:\n\n1. `batch_size`: This hyperparameter defines the number of independent sequences processed in parallel during each training iteration. It impacts the granularity of parameter updates and memory usage.\n\n2. `block_size`: This hyperparameter specifies the maximum length of sequences that the model can process. It determines the context length for predictions and affects the model's ability to capture dependencies within sequences.\n\n3. `max_iters`: This hyperparameter sets the maximum number of training iterations or epochs. It controls the duration of training and helps prevent overfitting by limiting the number of updates.\n\n4. `eval_interval`: This hyperparameter determines how often the model's performance is evaluated during training. It affects the frequency of monitoring training progress and validation performance.\n\n5. `learning_rate`: This hyperparameter controls the step size or rate at which the model parameters are updated during optimization. It influences the convergence speed and stability of the training process.\n\n6. `device`: This hyperparameter specifies the device (CPU or GPU) on which the model computations are performed. It allows for efficient utilization of available hardware resources.\n\n7. `eval_iters`: This hyperparameter determines the number of iterations between each evaluation of the model's performance. It can be used to reduce computational overhead during evaluation while still providing frequent updates on model performance.\n\n8. `n_embd`: This hyperparameter defines the dimensionality of the embedding layer. It determines the size of the vector representations for tokens in the input sequences.\n\n9. `n_head`: This hyperparameter specifies the number of attention heads in the multi-head attention mechanism used in the transformer architecture. It controls the model's ability to attend to different parts of the input sequence simultaneously.\n\n10. `n_layer`: This hyperparameter sets the number of transformer layers in the model. It determines the depth of the model and its capacity to capture complex patterns in the data.\n\n11. `dropout`: This hyperparameter defines the probability of dropping out neurons during training. It helps prevent overfitting by regularizing the model and reducing co-adaptation between neurons. A value of 0.0 means no dropout is applied, while higher values introduce more dropout.","metadata":{}},{"cell_type":"markdown","source":"## **Converting Strings Into Integers**","metadata":{}},{"cell_type":"code","source":"chars = sorted(list(set(text)))  # Get unique characters from the text and sort them\nvocab_size = len(chars)  # Total number of unique characters in the text\n\nprint(\"Unique characters that occur in this text :\\n\",chars)","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:26:46.289832Z","iopub.execute_input":"2024-02-17T21:26:46.290758Z","iopub.status.idle":"2024-02-17T21:26:46.300664Z","shell.execute_reply.started":"2024-02-17T21:26:46.290732Z","shell.execute_reply":"2024-02-17T21:26:46.299494Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Unique characters that occur in this text :\n ['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","output_type":"stream"}]},{"cell_type":"code","source":"# create a mapping from characters to integers\nstoi = {ch: i for i, ch in enumerate(chars)}  # Map each character to an integer index\nitos = {i: ch for i, ch in enumerate(chars)}  # Map each integer index to a character\n\n# Define encoding and decoding functions\nencode = lambda s: [stoi[c] for c in s]  # Encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # Decoder: take a list of integers, output a string","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:26:46.302447Z","iopub.execute_input":"2024-02-17T21:26:46.302729Z","iopub.status.idle":"2024-02-17T21:26:46.311274Z","shell.execute_reply.started":"2024-02-17T21:26:46.302706Z","shell.execute_reply":"2024-02-17T21:26:46.310372Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"- `stoi`:\n   - This function creates a mapping from characters to integers (`stoi` stands for \"string to integer\").\n   - It iterates over each character in the `chars` list and assigns a unique integer index to each character using the `enumerate` function.\n   - The resulting dictionary `stoi` maps each character to its corresponding integer index.\n\n- `itos`:\n   - This function creates a mapping from integers to characters (`itos` stands for \"integer to string\").\n   - It iterates over each integer index in the range of the length of `chars` and assigns each index to its corresponding character in the `chars` list.\n   - The resulting dictionary `itos` maps each integer index to its corresponding character.\n\n- `encode`:\n   - This function defines an encoding function (`encode`), which takes a string (`s`) as input and outputs a list of integers.\n   - Inside the lambda function, it iterates over each character (`c`) in the input string `s` and uses the `stoi` dictionary to convert each character to its corresponding integer index.\n\n- `decode`:\n   - This function defines a decoding function (`decode`), which takes a list of integers (`l`) as input and outputs a string.\n   - Inside the lambda function, it iterates over each integer (`i`) in the input list `l` and uses the `itos` dictionary to convert each integer index to its corresponding character.\n   - Finally, it joins the characters together using `join` to form the decoded string.","metadata":{}},{"cell_type":"markdown","source":"## **Test-Train Split**","metadata":{}},{"cell_type":"code","source":"# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)  # Convert the text to a tensor of integers\nn = int(0.9 * len(data))  # Calculate the index to split the data into train and validation sets (90% train, 10% validation)\ntrain_data = data[:n]  # Training data (first 90% of the data)\nval_data = data[n:]  # Validation data (remaining 10% of the data)","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:26:46.312616Z","iopub.execute_input":"2024-02-17T21:26:46.312881Z","iopub.status.idle":"2024-02-17T21:26:46.362065Z","shell.execute_reply.started":"2024-02-17T21:26:46.312859Z","shell.execute_reply":"2024-02-17T21:26:46.361179Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## **Data Loading**","metadata":{}},{"cell_type":"code","source":"def get_batch(split):\n    \"\"\"\n    Function to generate a small batch of data consisting of inputs x and targets y.\n\n    Args:\n    - split: A string indicating whether to use the training or validation data.\n\n    Returns:\n    - x: Input tensor of shape (batch_size, block_size) containing sequences of integers.\n    - y: Target tensor of shape (batch_size, block_size) containing sequences of integers shifted by one position.\n    \"\"\"\n    data = train_data if split == 'train' else val_data  # Select data based on the split (train or validation)\n    ix = torch.randint(len(data) - block_size, (batch_size,))  # Generate random indices for selecting sequences\n    x = torch.stack([data[i:i+block_size] for i in ix])  # Extract input sequences of length block_size\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # Extract target sequences shifted by one position\n    x, y = x.to(device), y.to(device)  # Move tensors to the specified device (CPU or GPU)\n    return x, y\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:26:46.363341Z","iopub.execute_input":"2024-02-17T21:26:46.363691Z","iopub.status.idle":"2024-02-17T21:26:46.371041Z","shell.execute_reply.started":"2024-02-17T21:26:46.363660Z","shell.execute_reply":"2024-02-17T21:26:46.369991Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## **Loss Function**","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    \"\"\"\n    Function to estimate the average loss on the training and validation data without performing gradient computation.\n\n    Returns:\n    - out: A dictionary containing the average loss for the training and validation data.\n    \"\"\"\n    out = {}  # Initialize a dictionary to store the output\n    model.eval()  # Set the model to evaluation mode (no gradient calculation)\n    for split in ['train', 'val']:  # Iterate over training and validation data\n        losses = torch.zeros(eval_iters)  # Initialize a tensor to store individual losses for each evaluation iteration\n        for k in range(eval_iters):  # Iterate over evaluation iterations\n            X, Y = get_batch(split)  # Get a batch of input-output pairs\n            logits, loss = model(X, Y)  # Forward pass through the model to get predictions and loss\n            losses[k] = loss.item()  # Store the loss value\n        out[split] = losses.mean()  # Calculate the mean loss for the current split and store it in the output dictionary\n    model.train()  # Set the model back to training mode\n    return out  # Return the dictionary containing the average losses for training and validation data","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:26:46.372421Z","iopub.execute_input":"2024-02-17T21:26:46.372761Z","iopub.status.idle":"2024-02-17T21:26:46.382195Z","shell.execute_reply.started":"2024-02-17T21:26:46.372730Z","shell.execute_reply":"2024-02-17T21:26:46.381373Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"The `estimate_loss` function serves to compute the average loss on both the training and validation datasets without engaging in gradient computation. By temporarily disabling gradient calculations and setting the model to evaluation mode, it iterates through the data splits, samples batches, and computes losses over multiple evaluation iterations. The function then returns a dictionary containing the mean losses for each split. This approach efficiently assesses the model's performance without updating its parameters and is crucial for monitoring training progress and model validation.","metadata":{}},{"cell_type":"markdown","source":"## **Model**","metadata":{}},{"cell_type":"code","source":"class Head(nn.Module):\n    \"\"\"\n    A single head of self-attention mechanism.\n\n    Args:\n    - head_size: The size of the attention head.\n\n    Attributes:\n    - key: Linear transformation for keys.\n    - query: Linear transformation for queries.\n    - value: Linear transformation for values.\n    - tril: Lower triangular mask to prevent attention to future tokens.\n    - dropout: Dropout layer.\n    \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        # Linear transformations for key, query, and value\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # Lower triangular mask to prevent attention to future tokens\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        # Dropout layer\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the self-attention mechanism.\n\n        Args:\n        - x: Input tensor of shape (batch_size, sequence_length, feature_dimension).\n\n        Returns:\n        - out: Output tensor after applying self-attention, of shape (batch_size, sequence_length, feature_dimension).\n        \"\"\"\n        B, T, C = x.shape  # Batch size, sequence length, and feature dimension\n        # Linear transformations for key, query, and value\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        \n        # Compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Mask future tokens\n        wei = F.softmax(wei, dim=-1)  # Apply softmax to get attention weights\n        wei = self.dropout(wei)  # Apply dropout\n        \n        # Perform the weighted aggregation of the values\n        v = self.value(x)  # (B,T,C)\n        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:26:46.386232Z","iopub.execute_input":"2024-02-17T21:26:46.386466Z","iopub.status.idle":"2024-02-17T21:26:46.398462Z","shell.execute_reply.started":"2024-02-17T21:26:46.386447Z","shell.execute_reply":"2024-02-17T21:26:46.397456Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"The `Head` class represents a single head of the self-attention mechanism used in the Transformer architecture. Below is an explanation of its architecture and functionality:\n\n- **Architecture**:\n  - **Linear Transformations**: The class initializes three linear transformations for keys, queries, and values, each of which maps the input tensor (`x`) from the feature dimension (`n_embd`) to the specified head size.\n  - **Lower Triangular Mask**: It creates a lower triangular mask (`tril`) as a buffer using PyTorch's `tril` function, ensuring that during the attention computation, the model does not attend to future tokens in the sequence.\n  - **Dropout Layer**: A dropout layer is applied to the attention weights (`wei`) to regularize the model and prevent overfitting.\n\n- **Functionality**:\n  - **Forward Pass**: In the `forward` method, the input tensor (`x`) is passed through the linear transformations to obtain the keys (`k`), queries (`q`), and values (`v`).\n  - **Attention Computation**: The attention scores, also known as affinities, are computed by performing a matrix multiplication between queries and keys, scaled by the square root of the feature dimension (`C`). The resulting attention weights (`wei`) are masked to prevent attention to future tokens using the lower triangular mask and then normalized using softmax to obtain valid attention probabilities.\n  - **Weighted Aggregation**: The values (`v`) are weighted by the attention probabilities (`wei`) and aggregated using matrix multiplication to produce the output tensor (`out`), representing the attended features.","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \"\"\"\n    Multi-head self-attention mechanism.\n\n    Args:\n    - num_heads: The number of attention heads.\n    - head_size: The size of each attention head.\n\n    Attributes:\n    - heads: List of attention heads.\n    - proj: Linear transformation for projecting concatenated attention heads.\n    - dropout: Dropout layer.\n    \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        # Create multiple attention heads\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # Linear transformation for projecting concatenated attention heads\n        self.proj = nn.Linear(n_embd, n_embd)\n        # Dropout layer\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the multi-head self-attention mechanism.\n\n        Args:\n        - x: Input tensor of shape (batch_size, sequence_length, feature_dimension).\n\n        Returns:\n        - out: Output tensor after applying multi-head self-attention, of shape (batch_size, sequence_length, feature_dimension).\n        \"\"\"\n        # Apply each attention head in parallel and concatenate the outputs\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        # Project the concatenated output\n        out = self.dropout(self.proj(out))\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:26:46.402068Z","iopub.execute_input":"2024-02-17T21:26:46.402321Z","iopub.status.idle":"2024-02-17T21:26:46.412392Z","shell.execute_reply.started":"2024-02-17T21:26:46.402300Z","shell.execute_reply":"2024-02-17T21:26:46.411623Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"The `MultiHeadAttention` class implements a multi-head self-attention mechanism, a crucial component of the Transformer architecture. Below is an explanation of its architecture and functionality:\n\n- **Architecture**:\n  - **Multiple Attention Heads**: The class initializes multiple attention heads (`heads`) using a `ModuleList`, each with the specified `head_size`. The number of attention heads is determined by the `num_heads` parameter.\n  - **Projection Layer**: After applying each attention head in parallel, the outputs are concatenated and passed through a linear transformation (`proj`). This projection layer helps in combining information from multiple attention heads.\n  - **Dropout Layer**: To regularize the model and prevent overfitting, a dropout layer is applied after the projection layer.\n\n- **Functionality**:\n  - **Forward Pass**: In the `forward` method, the input tensor (`x`) is passed through each attention head in parallel. The outputs from all attention heads are concatenated along the feature dimension.\n  - **Concatenation**: The outputs from different attention heads are concatenated along the last dimension (`dim=-1`), resulting in a tensor with increased feature dimensionality.\n  - **Projection**: The concatenated tensor is then projected back to the original feature dimensionality using a linear transformation (`proj`). This step helps maintain the desired feature dimensionality and facilitates information integration from multiple attention heads.\n  - **Dropout**: Finally, dropout is applied to the projected tensor to regularize the model and mitigate overfitting.","metadata":{}},{"cell_type":"code","source":"class FeedFoward(nn.Module):\n    \"\"\"\n    Feedforward neural network composed of linear layers followed by a non-linearity and dropout.\n\n    Args:\n    - n_embd: The input and output dimension of the linear layers.\n\n    Attributes:\n    - net: Sequential module containing linear layers, ReLU activation, and dropout.\n    \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        # Define a sequential neural network module\n        self.net = nn.Sequential(\n            # First linear layer followed by ReLU activation\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            # Second linear layer\n            nn.Linear(4 * n_embd, n_embd),\n            # Dropout layer\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the feedforward neural network.\n\n        Args:\n        - x: Input tensor of shape (batch_size, sequence_length, feature_dimension).\n\n        Returns:\n        - out: Output tensor after applying the feedforward network, of shape (batch_size, sequence_length, feature_dimension).\n        \"\"\"\n        return self.net(x)","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:26:46.413774Z","iopub.execute_input":"2024-02-17T21:26:46.414087Z","iopub.status.idle":"2024-02-17T21:26:46.424767Z","shell.execute_reply.started":"2024-02-17T21:26:46.414063Z","shell.execute_reply":"2024-02-17T21:26:46.423648Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"The `FeedForward` class defines a feedforward neural network architecture composed of linear layers followed by a non-linearity (ReLU activation) and dropout. Here's an explanation of its architecture:\n\n- **Architecture**:\n  - **Sequential Module**: The class initializes a `Sequential` module containing a sequence of operations applied sequentially to the input tensor.\n  - **Linear Layers**: Two linear layers are defined within the sequential module. \n    - The first linear layer takes an input of dimension `n_embd` and outputs a tensor of dimension `4 * n_embd`.\n    - The second linear layer takes the output of the first layer (with dimension `4 * n_embd`) and maps it back to the original input dimensionality (`n_embd`).\n  - **Activation Function**: Between the linear layers, a Rectified Linear Unit (ReLU) activation function is applied element-wise. ReLU introduces non-linearity to the network, allowing it to learn complex mappings from input to output.\n  - **Dropout Layer**: After the second linear layer, dropout is applied. Dropout randomly sets a fraction of input units to zero during training, which helps prevent overfitting by reducing the model's reliance on specific units.\n  \n- **Functionality**:\n  - **Forward Pass**: In the `forward` method, the input tensor (`x`) is passed through the sequential module, which sequentially applies the linear layers, ReLU activation, and dropout.\n  - **Output**: The output tensor after passing through the feedforward network has the same shape as the input tensor (`(batch_size, sequence_length, feature_dimension)`), with each element representing the corresponding feature in the input sequence transformed by the feedforward network.","metadata":{}},{"cell_type":"code","source":"class Block(nn.Module):\n    \"\"\"\n    Transformer block\n\n    Args:\n    - n_embd: The embedding dimension.\n    - n_head: The number of attention heads.\n\n    Attributes:\n    - sa: Multi-head self-attention module.\n    - ffwd: Feedforward neural network module.\n    - ln1: Layer normalization module.\n    - ln2: Layer normalization module.\n    \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        \"\"\"\n        Initialize the Transformer block.\n\n        Args:\n        - n_embd: The embedding dimension.\n        - n_head: The number of attention heads.\n        \"\"\"\n        super().__init__()\n        # Calculate the size of each attention head\n        head_size = n_embd // n_head\n        # Multi-head self-attention module\n        self.sa = MultiHeadAttention(n_head, head_size)\n        # Feedforward neural network module\n        self.ffwd = FeedFoward(n_embd)\n        # Layer normalization module\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the Transformer block.\n\n        Args:\n        - x: Input tensor of shape (batch_size, sequence_length, feature_dimension).\n\n        Returns:\n        - out: Output tensor after applying the Transformer block, of shape (batch_size, sequence_length, feature_dimension).\n        \"\"\"\n        # Apply multi-head self-attention followed by layer normalization and residual connection\n        x = x + self.sa(self.ln1(x))\n        # Apply feedforward network followed by layer normalization and residual connection\n        x = x + self.ffwd(self.ln2(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:26:46.425828Z","iopub.execute_input":"2024-02-17T21:26:46.426124Z","iopub.status.idle":"2024-02-17T21:26:46.438256Z","shell.execute_reply.started":"2024-02-17T21:26:46.426069Z","shell.execute_reply":"2024-02-17T21:26:46.437477Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"The `Block` class represents a single Transformer block, which is a fundamental building block of the Transformer architecture. Below is an explanation of its architecture:\n\n- **Architecture**:\n  - **Multi-Head Self-Attention Module**: The block initializes a multi-head self-attention module (`sa`), which consists of multiple attention heads operating in parallel. Each attention head independently attends to different parts of the input sequence, allowing the model to capture long-range dependencies efficiently.\n  - **Feedforward Neural Network Module**: It also initializes a feedforward neural network module (`ffwd`), which consists of linear layers followed by ReLU activation and dropout. This component introduces non-linearity and enables the model to capture complex patterns in the data.\n  - **Layer Normalization Modules**: Two layer normalization modules (`ln1` and `ln2`) are initialized. Layer normalization normalizes the activations of each layer, helping stabilize the training process and improve model performance.\n  \n- **Functionality**:\n  - **Forward Pass**: In the `forward` method, the input tensor (`x`) is passed through the multi-head self-attention module (`sa`). The output is then passed through layer normalization (`ln1`) and added to the input tensor (`x`) to form a residual connection.\n  - **Feedforward Network**: Next, the output from the attention module is passed through the feedforward neural network (`ffwd`). Again, the output is passed through layer normalization (`ln2`) and added to the previous output to form another residual connection.\n  - **Output**: The final output of the Transformer block represents the processed input tensor, capturing both the self-attention and feedforward network transformations.","metadata":{}},{"cell_type":"code","source":"class BigramLanguageModel(nn.Module):\n    \"\"\"\n    Super simple bigram language model.\n\n    Attributes:\n    - token_embedding_table: Embedding layer for token embeddings.\n    - position_embedding_table: Embedding layer for position embeddings.\n    - blocks: Sequential module of Transformer blocks.\n    - ln_f: Layer normalization for the final layer.\n    - lm_head: Linear layer for language modeling.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # Embedding layer for token embeddings\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        # Embedding layer for position embeddings\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # Sequential module of Transformer blocks\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        # Layer normalization for the final layer\n        self.ln_f = nn.LayerNorm(n_embd)\n        # Linear layer for language modeling\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        \"\"\"\n        Forward pass of the bigram language model.\n\n        Args:\n        - idx: Input tensor of shape (batch_size, sequence_length) containing token indices.\n        - targets: Target tensor of shape (batch_size, sequence_length) containing target token indices.\n\n        Returns:\n        - logits: Logits tensor of shape (batch_size, sequence_length, vocab_size) containing predicted logits.\n        - loss: Optional loss tensor computed using cross-entropy if targets are provided.\n        \"\"\"\n        B, T = idx.shape\n\n        # Token embeddings\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        # Position embeddings\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        # Add token and position embeddings\n        x = tok_emb + pos_emb  # (B,T,C)\n        # Pass through Transformer blocks\n        x = self.blocks(x)  # (B,T,C)\n        # Apply layer normalization\n        x = self.ln_f(x)  # (B,T,C)\n        # Linear layer for language modeling\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n\n        # Calculate loss if targets are provided\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B * T, C)\n            targets = targets.view(B * T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        \"\"\"\n        Generate new tokens using the bigram language model.\n\n        Args:\n        - idx: Input tensor of shape (batch_size, sequence_length) containing token indices.\n        - max_new_tokens: Maximum number of new tokens to generate.\n\n        Returns:\n        - idx: Tensor containing the input tokens extended with generated tokens, of shape (batch_size, sequence_length + max_new_tokens).\n        \"\"\"\n        # Iterate for max_new_tokens iterations\n        for _ in range(max_new_tokens):\n            # Crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # Get the predictions\n            logits, _ = self(idx_cond)\n            # Focus only on the last time step\n            logits = logits[:, -1, :]  # (B, C)\n            # Apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # Sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # Append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n        return idx","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:26:46.439415Z","iopub.execute_input":"2024-02-17T21:26:46.439724Z","iopub.status.idle":"2024-02-17T21:26:46.454006Z","shell.execute_reply.started":"2024-02-17T21:26:46.439702Z","shell.execute_reply":"2024-02-17T21:26:46.453067Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"The `BigramLanguageModel` class represents a simple bigram language model based on the Transformer architecture. Here's an explanation of its architecture and functionality:\n\n- **Architecture**:\n  - **Token Embedding Layer**: Initializes an embedding layer (`token_embedding_table`) to map token indices to dense vector representations. The size of the embedding matrix is determined by the vocabulary size (`vocab_size`) and the embedding dimension (`n_embd`).\n  - **Position Embedding Layer**: Creates another embedding layer (`position_embedding_table`) to encode positional information into the input tokens. This layer assigns a unique position embedding to each token position in the sequence.\n  - **Transformer Blocks**: Utilizes a sequence of Transformer blocks (`blocks`) to process the input sequence. Each block consists of multi-head self-attention followed by feedforward neural network layers, facilitating the capture of contextual dependencies and patterns within the input sequence.\n  - **Layer Normalization**: Applies layer normalization (`ln_f`) after the Transformer blocks to stabilize the learning process and improve model performance.\n  - **Linear Layer for Language Modeling**: Defines a linear layer (`lm_head`) to project the output of the Transformer blocks to the vocabulary size, producing logits for each token in the vocabulary.\n\n- **Functionality**:\n  - **Forward Pass**: In the `forward` method, the input tensor (`idx`) containing token indices is passed through the token embedding layer and added to position embeddings. The resulting tensor is then processed by the Transformer blocks and layer normalization, followed by the linear layer to compute logits for language modeling. If targets are provided, the method also computes the cross-entropy loss.\n  - **Token Generation**: The `generate` method generates new tokens by iteratively predicting the next token based on the previous sequence. It repeatedly samples from the softmax distribution of logits for the last token and appends the sampled token to the sequence until the specified maximum number of new tokens is generated.","metadata":{}},{"cell_type":"markdown","source":"## **Training**","metadata":{}},{"cell_type":"code","source":"model = BigramLanguageModel()  # Initialize the BigramLanguageModel\nm = model.to(device)  # Move the model to the specified device (CPU or GPU)\n# Print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters()) / 1e6, 'M parameters')\n\n# Create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# Main training loop\nfor iter in range(max_iters):\n\n    # Every once in a while, evaluate the loss on train and validation sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        # Estimate loss on train and val sets\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # Sample a batch of data\n    xb, yb = get_batch('train')\n\n    # Evaluate the loss\n    logits, loss = model(xb, yb)\n    # Zero the gradients before the backward pass\n    optimizer.zero_grad(set_to_none=True)\n    # Backpropagation: compute gradients\n    loss.backward()\n    # Update model parameters\n    optimizer.step()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:26:46.455440Z","iopub.execute_input":"2024-02-17T21:26:46.456108Z","iopub.status.idle":"2024-02-17T21:30:38.918771Z","shell.execute_reply.started":"2024-02-17T21:26:46.456077Z","shell.execute_reply":"2024-02-17T21:30:38.917604Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"0.209213 M parameters\nstep 0: train loss 4.3193, val loss 4.3138\nstep 100: train loss 2.5515, val loss 2.5568\nstep 200: train loss 2.4189, val loss 2.4242\nstep 300: train loss 2.3738, val loss 2.3882\nstep 400: train loss 2.3395, val loss 2.3455\nstep 500: train loss 2.3080, val loss 2.3140\nstep 600: train loss 2.2704, val loss 2.2786\nstep 700: train loss 2.2340, val loss 2.2375\nstep 800: train loss 2.2004, val loss 2.2153\nstep 900: train loss 2.1841, val loss 2.2072\nstep 1000: train loss 2.1548, val loss 2.1685\nstep 1100: train loss 2.1148, val loss 2.1354\nstep 1200: train loss 2.1044, val loss 2.1290\nstep 1300: train loss 2.0781, val loss 2.1119\nstep 1400: train loss 2.0639, val loss 2.0925\nstep 1500: train loss 2.0460, val loss 2.0793\nstep 1600: train loss 2.0279, val loss 2.0630\nstep 1700: train loss 2.0083, val loss 2.0587\nstep 1800: train loss 2.0144, val loss 2.0471\nstep 1900: train loss 1.9851, val loss 2.0272\nstep 2000: train loss 1.9826, val loss 2.0196\nstep 2100: train loss 1.9646, val loss 2.0077\nstep 2200: train loss 1.9510, val loss 1.9979\nstep 2300: train loss 1.9445, val loss 1.9937\nstep 2400: train loss 1.9365, val loss 1.9870\nstep 2500: train loss 1.9276, val loss 1.9815\nstep 2600: train loss 1.9102, val loss 1.9811\nstep 2700: train loss 1.9088, val loss 1.9597\nstep 2800: train loss 1.9075, val loss 1.9535\nstep 2900: train loss 1.8946, val loss 1.9402\nstep 3000: train loss 1.8924, val loss 1.9452\nstep 3100: train loss 1.8666, val loss 1.9371\nstep 3200: train loss 1.8748, val loss 1.9288\nstep 3300: train loss 1.8635, val loss 1.9294\nstep 3400: train loss 1.8620, val loss 1.9280\nstep 3500: train loss 1.8420, val loss 1.9209\nstep 3600: train loss 1.8547, val loss 1.9221\nstep 3700: train loss 1.8422, val loss 1.9033\nstep 3800: train loss 1.8331, val loss 1.9030\nstep 3900: train loss 1.8282, val loss 1.9048\nstep 4000: train loss 1.8268, val loss 1.8984\nstep 4100: train loss 1.8207, val loss 1.8999\nstep 4200: train loss 1.8188, val loss 1.8996\nstep 4300: train loss 1.8072, val loss 1.8784\nstep 4400: train loss 1.8037, val loss 1.8854\nstep 4500: train loss 1.7988, val loss 1.8873\nstep 4600: train loss 1.8026, val loss 1.8760\nstep 4700: train loss 1.7925, val loss 1.8671\nstep 4800: train loss 1.7944, val loss 1.8699\nstep 4900: train loss 1.7894, val loss 1.8741\nstep 4999: train loss 1.7761, val loss 1.8608\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **Output Of The Model**","metadata":{}},{"cell_type":"code","source":"# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:30:38.919932Z","iopub.execute_input":"2024-02-17T21:30:38.920373Z","iopub.status.idle":"2024-02-17T21:30:54.944397Z","shell.execute_reply.started":"2024-02-17T21:30:38.920346Z","shell.execute_reply":"2024-02-17T21:30:54.943350Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\nghts ther whe noth thast hup ase stenpetters,\nBut disse sselove stlove:\n\nOrlof anon dects chires, thy and morede par to dote,\n\nAght file yought ey a to thou bed,\nBut truthers,\nAnbe thier with, and, chame do oth I conth '(but ought sure-deariose of stroomfed,\nThat you shee lecestin reaked, owee proos, mons,\nBut thunteounore bed To xeye, as un poince,\nWithater,\nHake shunce, as my com, thering.\nA min's bewhints, thou heyu, many time waity\nNo might a jeccht telf to bull\nBlow ve mights be ge\nRow to  tirel, thend phold ame (heal I now anloove wound in wed mu adt I sell.\nTheid see, rut weet tong kned,\nMut so swa to, sifter orean, apn's suble on 'se lisgeauty ser in \nBust of that I lies so fe whit, freark,\nThat ond frror poonte, law\nBy thou peaume shall my four's baken thens!\nDoth yos houghts coproong my alo', an thing preidn,\nSo leagure forgh bettome then muse)\nTo weart my swing deor exind pay:\nAs ching my sunlie) feter in no scor in herwine,\nWroms hightae oo love.  \nFrighiing bearks thy sun'rive troue,\nBearnow, to me bay thee theangrt firs,\nMy timing haveng coof wningsuld whath swith tancry seep,\nWhy imunging hou a lieve nears my by now,\nMet Aris herstent it ar worsed: \nSo eyor, thou kined that hase rovings,\nTo sualligich lime loves fariors eve\nLour'swich's fraasterss unlacgns,\nTo pencont, my hold I walls weagin in heor-somes,\nThenbs reas that and bese aleart:\nI apk to and wearnet ise blaing a oaingete\nTo thime days lie prome, reoun.\n\nFor mexgiqut my min's git skeaunn the hee daince,\nAnd doably fer mores ande fe.  \n\n\nFo ong your, rinfy fhit sairds mayks vyour whirmath fingres\nSight of be hin is beanitys never appeage agays:\nThe onow you me send thalls be cairty shen vail day,\nThou hou ad the hachencis shed, whate crue anon groot hewdserong my to wo nights thou dorst,\nThat his siab\nO's may footh nece In arven, I high eake and net withapt.\n\n\n\n\nOr strign me he mins by here:\nWheere thy ponomm whith then is hat hin to worth inme ewet bestink.\n\nI srup ric thy eard,\nOr boor that\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **Results**\n\n#### The results are very bad as you can see but it has captured some patterns in the test and the struture of sentence is more human like. If we make the model more complex we will surely get better results.","metadata":{}}]}